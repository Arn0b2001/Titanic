# -*- coding: utf-8 -*-
"""CSE422 project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HGv_Tdf25lM3W3eiocPgPEQh-pPSGXYq
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

from sklearn.preprocessing import StandardScaler

T_data=pd.read_csv('/content/drive/MyDrive/Untitled folder/titanic.csv')
T_data.head(7)

"""**Data Analysis**"""

T_data.shape

T_data=T_data.drop(["PassengerId", "Name"] ,axis=1)

#Check for null values
T_data.isnull().sum()

"""There exists null values so we have to handle it


"""

#checking the percentage of missing values
print((T_data.isnull().sum() * 100) / len(T_data))

"""**Lets handle those missing value**"""

#We can see from the percentege that almost 80% values in Cabin is none
T_data=T_data.drop(columns=['Cabin'],axis=1)

#If the colum is numerical values we can replace the null values by using mean or median
#But first we have to decide weather to use meaan or median
meanA = T_data['Age'].mean()
meanF = T_data['Fare'].mean()

medianA = T_data['Age'].median()
medianF = T_data['Fare'].median()
print(f'''AgeMean: {meanA}
AgeMedian: {medianA}''')
print(f'''FareMean: {meanF}
FareMedian: {medianF}''')

# we do not undertand which one should we use
T_data['Age_mean']=T_data['Age'].fillna(meanA)
T_data['Age_median']=T_data['Age'].fillna(medianA)
T_data['Fare_mean']=T_data['Fare'].fillna(meanF)
T_data['Fare_median']=T_data['Fare'].fillna(medianF)

T_data.head(2)

print("Original :",T_data['Age'].var())
print("Age after mean imputation:",T_data['Age_mean'].var())
print("Age after median imputation:",T_data['Age_median'].var())

print("Original :",T_data['Fare'].var())
print("Fare after mean imputation:",T_data['Fare_mean'].var())
print("Fare after median imputation:",T_data['Fare_median'].var())

#For more visualization mean
T_data['Age'].plot(kind = "kde",label="Original")

T_data['Age_mean'].plot(kind = "kde",label = "Mean")

T_data['Age_median'].plot(kind = "kde",label = "Median")

plt.legend()
plt.show()

T_data['Fare'].plot(kind = "kde",label="Original")

T_data['Fare_mean'].plot(kind = "kde",label = "Mean")

T_data['Fare_median'].plot(kind = "kde",label = "Median")

plt.legend()
plt.show()

#Using mean we are handling the null values
T_data['Age'] = T_data['Age'].fillna(T_data['Age'].mean())
T_data['Fare'] = T_data['Fare'].fillna(T_data['Fare'].mean())

T_data.isnull().sum()

T_data.head()

"""## Encoding"""

# We have found Sex and Embarked column is categorical and it is important or not we do not know so we have to do encoding task
T_data['Sex'].value_counts()

#We can use One Hot Encoding

T_data['Sex'] = pd.get_dummies(T_data['Sex'], drop_first=True)

T_data['Embarked'].value_counts()

#Using label encoding
T_data.replace({'Embarked':{'S':0, 'Q': 1, 'C':2}}, inplace = True)

T_data.head()

correlation_matrix = T_data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm")
plt.show()

"""splitting into data and labels"""

x=T_data.drop(["Survived","Age_mean", "Age_median", "Fare_mean", "Fare_median","Ticket"] ,axis=1)
y=T_data["Survived"]

T_data['Survived'].value_counts()

class_counts = T_data['Survived'].value_counts()

# Create a bar chart to visualize class imbalance
plt.bar(class_counts.index, class_counts.values, tick_label=['Not Survived', 'Survived'])
plt.xlabel('Survived')
plt.ylabel('Count')
plt.title('Class Distribution of Survived')
plt.show()

print(x.shape)

print(y.shape)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3,stratify=y, random_state = 2)

print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

"""**Model training without scaling**"""

models = [LogisticRegression(max_iter = 1000), RandomForestClassifier(), SVC(), KNeighborsClassifier()]
model_names = ['Logistic Regression', 'Random Forest', 'SVM', 'KNN']

accuracies = []
precisions = []
recalls = []
f1_scores = []

for model in models:
    classifier = model
    classifier.fit(x_train, y_train)
    test_prediction = classifier.predict(x_test)
#///////////////////////////////////////////////////////////////////////////////
    # Calculate metrics
    test_accuracy = accuracy_score(test_prediction, y_test)
    accuracies.append(test_accuracy)
    print(model,test_accuracy )

    precision = precision_score(y_test, test_prediction)
    precisions.append(precision)

    recall = recall_score(y_test, test_prediction)
    recalls.append(recall)

    f1 = f1_score(y_test, test_prediction)
    f1_scores.append(f1)

    # Create confusion matrix
    cm = confusion_matrix(y_test, test_prediction)

    # Plot confusion matrix as a heatmap
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
    plt.title(f'Confusion Matrix - {model.__class__.__name__}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# Plotting the bar chart comparing model metrics
plt.figure(figsize=(18, 6))

plt.subplot(2, 2, 1)
plt.bar(model_names, accuracies, color='skyblue')
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Accuracy Comparison')

plt.subplot(2, 2, 2)
plt.bar(model_names, precisions, color='salmon')
plt.xlabel('Models')
plt.ylabel('Precision')
plt.title('Precision Comparison')

plt.subplot(2, 2, 3)
plt.bar(model_names, recalls, color='lightgreen')
plt.xlabel('Models')
plt.ylabel('Recall')
plt.title('Recall Comparison')

plt.subplot(2, 2, 4)
plt.bar(model_names, f1_scores, color='gold')
plt.xlabel('Models')
plt.ylabel('F1 Score')
plt.title('F1 Score Comparison')

plt.tight_layout()
plt.show()

"""**Model training with scaling**"""

scaler = StandardScaler()

x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

models = [LogisticRegression(max_iter = 1000), RandomForestClassifier(), SVC(), KNeighborsClassifier()]
model_names = ['Logistic Regression', 'Random Forest', 'SVM', 'KNN']

accuracies = []
precisions = []
recalls = []
f1_scores = []

for model in models:
    classifier = model
    classifier.fit(x_train_scaled, y_train)
    test_prediction = classifier.predict(x_test_scaled)
#///////////////////////////////////////////////////////////////////////////////
    # Calculate metrics
    test_accuracy = accuracy_score(test_prediction, y_test)
    accuracies.append(test_accuracy)
    print(model,test_accuracy )

    precision = precision_score(y_test, test_prediction)
    precisions.append(precision)

    recall = recall_score(y_test, test_prediction)
    recalls.append(recall)

    f1 = f1_score(y_test, test_prediction)
    f1_scores.append(f1)

    # Create confusion matrix
    cm = confusion_matrix(y_test, test_prediction)

    # Plot confusion matrix as a heatmap
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
    plt.title(f'Confusion Matrix - {model.__class__.__name__}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# Plotting the bar chart comparing model metrics
plt.figure(figsize=(18, 6))

plt.subplot(2, 2, 1)
plt.bar(model_names, accuracies, color='skyblue')
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Accuracy Comparison')

plt.subplot(2, 2, 2)
plt.bar(model_names, precisions, color='salmon')
plt.xlabel('Models')
plt.ylabel('Precision')
plt.title('Precision Comparison')

plt.subplot(2, 2, 3)
plt.bar(model_names, recalls, color='lightgreen')
plt.xlabel('Models')
plt.ylabel('Recall')
plt.title('Recall Comparison')

plt.subplot(2, 2, 4)
plt.bar(model_names, f1_scores, color='gold')
plt.xlabel('Models')
plt.ylabel('F1 Score')
plt.title('F1 Score Comparison')

plt.tight_layout()
plt.show()